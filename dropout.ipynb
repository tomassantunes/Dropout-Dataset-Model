{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição dos conjuntos de treino, teste e modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"test-files/dropout-trabalho2.csv\")\n",
    "X_train = data_train.drop([data_train.columns[0]], axis = 1)\n",
    "y_train = data_train[data_train.columns[0]]\n",
    "\n",
    "data_test = pd.read_csv(\"test-files/dropout-test-modelo.csv\")\n",
    "X_test = data_test.drop([data_test.columns[0]], axis = 1)\n",
    "y_test = data_test[data_test.columns[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação classe modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelo:\n",
    "\n",
    "    def predict(X):\n",
    "        mnb = MultinomialNB()\n",
    "\n",
    "        mnb.fit(X_train, y_train)\n",
    "\n",
    "        return mnb.predict(X) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict do modelo, testes de precisão e cobertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label. It should be one of [131385939, 131391397]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pred \u001b[39m=\u001b[39m modelo\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCobertura: \u001b[39m\u001b[39m{\u001b[39;00mrecall_score(y_test, pred, pos_label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1920\u001b[0m, in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecall_score\u001b[39m(\n\u001b[1;32m   1790\u001b[0m     y_true,\n\u001b[1;32m   1791\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1798\u001b[0m ):\n\u001b[1;32m   1799\u001b[0m     \u001b[39m\"\"\"Compute the recall.\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m \n\u001b[1;32m   1801\u001b[0m \u001b[39m    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[39m    array([1. , 1. , 0.5])\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1920\u001b[0m     _, r, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1921\u001b[0m         y_true,\n\u001b[1;32m   1922\u001b[0m         y_pred,\n\u001b[1;32m   1923\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1924\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1925\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1926\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1927\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1928\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1929\u001b[0m     )\n\u001b[1;32m   1930\u001b[0m     \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1563\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1372\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[39mif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m present_labels:\n\u001b[1;32m   1371\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(present_labels) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 1372\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1373\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpos_label=\u001b[39m\u001b[39m{\u001b[39;00mpos_label\u001b[39m}\u001b[39;00m\u001b[39m is not a valid label. It \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshould be one of \u001b[39m\u001b[39m{\u001b[39;00mpresent_labels\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m             )\n\u001b[1;32m   1376\u001b[0m     labels \u001b[39m=\u001b[39m [pos_label]\n\u001b[1;32m   1377\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label. It should be one of [131385939, 131391397]"
     ]
    }
   ],
   "source": [
    "pred = modelo.predict(X_test)\n",
    "\n",
    "print(f\"Cobertura: {recall_score(y_test, pred, pos_label=1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
